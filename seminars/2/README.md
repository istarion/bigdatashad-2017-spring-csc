## Семинар 2.
### Простые MapReduce программы

На данном семинаре будут рассмотрены:

 - Ряд идей и их реализаций в виде MapReduce программ для расчета метрик первой группы из hw1.

После данного семинара студент должен уметь:

- Регулярно запускать задачи по cron
- Написать программы для расчета ряда простых метрик

#### Инфраструктура
Для запуска программ по расписанию служит программа `cron`. Для изменения расписания введите команду:

    $ crontab -e
Файл расписания откроется в консольном редакторе. Редактори вы можете выбирать, например:

    $ EDITOR=vim crontab -e
    $ EDITOR=nano crontab -e
Каждое задание, которое надо запускать - в отдельной строке. В строке через пробел указываются: минута, час, день, месяц, день недели и собственно команда. Вот пример запуска моего скрипта для hw1:

    */30 3-5 * * * /home/agorokhov/devel/shad/hw1/runner.sh `date -d "-1 day" +\%F`
т.е. скрипт запускается каждые 30 минут, с 3 до 5 часов ночи. В параметрах скрипту передается вчерашняя дата, которая получается командой `date`:

    $ date -d "-1 day" +%F
Обратите внимание, что в cron'е символ '%' нужно экранировать. Подробнее про формат таких файлов тут:

    $ man 5 crontab
Все, что выводит запущенная cron'ом программа в stderr и stdout можно прочитать в своей почте по команде `mail`. Вводите там номер письма или Ctrl-D для выхода.


#### MapReduce
1. Решим задачу нахождения уникальных посетителей в логе. Нам понадобится вытащить из строк лога идентификаторы посетителей, потом оставить только уникальные. Если хочется по минимуму использовать память, то можно сделать так:

        $ cat access.log | ./get_user_id.py | sort | uniq
В этой команде get_user_id.py преобразует строки лога (и выдает user_id). Команда uniq работает на сортированных данных и оставляет уникальные.

2. Перенесем задачу на Hadoop. get_user_id.py может работать параллельно и независимо на разных данных - это маппер. uniq агрегирует сортированные записи - редьюсер. Сортировку обеспечит Hadoop, поэтому тот же результат можно получить так:

        $ hadoop --config conf.empty
            jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar
            -files get_user_id.py
            -input access.log
            -output out/
            -mapper ./get_user_id.py
            -reducer uniq
    Тут:

    - hadoop запускается в standalone режиме, т.е. на этом же сервере и с локальной файловой системой - это указано в параметре `--config`
    - указан специальный jar файл для работы hadoop в режиме streaming
    - в -files перечислены пути файлов, которые потребуются для запуска задачи
    - потом - входной и выходной путь
    - далее - команда, которая будет работать на map-стадии
    - и, наконец, команда для reduce-стадии.

    Результат будет в файле `out/part-r-00000` такой же, как и полученный ранее без hadoop в п.1.

3. При запуске этой команды hadoop копирует все файлы из `-files` на рабочие машины кластера (в standalone режиме это текущий сервер) в рабочую директорию задачи. Т.е. в `-files` перечисляются полные пути к файлам на нашем сервере. А в -mapper и -reducer указывают команды относительно рабочей директории задачи, поэтому там нет полных путей.

    Входные и выходные директории в standalone режиме располагаются на нашем рабочем сервере, а в распределенном режиме - на кластере, т.е. в HDFS. Для включения распределенного режима параметр `--config` надо убрать, чтобы использовалась конфигурация из `/etc/conf/hadoop` - она "смотрит" на кластер.

    Редьюсер uniq в данном случае - команда, которая есть на каждой ноде кластера. Можно реализовать её самому, например как в uniq.py. Тогда этот файл тоже надо будет прилодить к задаче.

    Документация по streaming: http://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html

4. Советы по отладке:

    - отлаживайте локально, т.е. в standalone, на сэмпле данных
    - отлаживайте на сэмпле, но уже на кластере
    - можно запускать только маппер, если установить 0 редьюсеров: `-D mapreduce.job.reduces=0`
    - вывод в stderr будет на консоли в standalone режиме; при запуске на кластере -  в веб-интерфейсе ResourceManager (http://hadoop2-10.yandex.ru:8088/cluster)

5. Добавим логики в маппер - разбор строки с помощью регулярного выражения и фильтрация кодов ответа 200 - `get_user_id_re.py`. В редьюсер добавим подсчет числа хитов для каждого пользователя - `hits_per_id.py`. Сравните его в uniq.py, обратите внимание на их сходство и на необходость вывода последнего ключа после цикла в `hits_per_id.py`. Напишите команду для Hadoop аналогично той, что в п.2. Проверьте, что результат совпадает с полученным такой командой (с точностью до формата вывода):

        $ cat access.log | ./get_user_id_re.py | sort | uniq -с
или:

        $ cat access.log | ./get_user_id_re.py | sort | ./hits_per_id.py
Да, очень удобно, когда себя можно проверить аналогичной локальной командой.

6. Чуть более сложная задача - подсчет сессий. Надо для каждого пользователя отсортировать события по хронологии. Можно сделать это в памяти редьюсера. А если таких событий много? Тогда проще сделать так - пусть сортирует Hadoop, т.е. ключом будет пара (user_id, timestamp). Только разделять по редьюсерам надо только по первому полю ключа, чтобы все записи одного пользователя попали на один редьюсер. Такой прием называется secondary sort.

    Для этого в команду добавляем настройки сравнения, где указываем специальный класс компаратора, ключ из двух полей и сравнение - по этим двум полям:

        -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator
        -D stream.num.map.output.key.fields=2
        -D mapred.text.key.comparator.options=-k1,2
Также для правильного распределение по редьюсерам используем специальный партишенер и его настройку:

        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
        -D mapred.text.key.partitioner.options=-k1,1
Вся команда есть в `secondary_sort.py`, там не хватает только редьюсера. Напишите его самостоятельно, пусть для каждого user_id он выводит 2 числа: число хитов и число сессий. Вот так за один проход можно считать сразу две статистики.

7. Посчитаем теперь число хитов на каждый url. Для этого можно использовать команду из п.5. Поменять придется только маппер, чтобы он выдавал не user_id, а url, редьюсер же останется без изменений.

8. Теперь найдем наиболее популярные страницы. Для этого результат из п.7 надо отсортировать по количеству. Для получения глобально отсортированного вывода можно указать в параметрах ровно один редьюсер:
            `-D mapreduce.job.reduces=1`
Если указывать большее число, то работать это будет эффективнее, но полностью сортированного множества не получится. Для получение топа записей придется читать каждый выходной файл и сливать из вместе.

    Для нужной сортировки надо число посещений сделать ключом, указать для Hadoop правила сравнения (как числа и в порядке убывания):

        -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator
        -D mapred.text.key.comparator.options=-nr
    Полная документация по классу KeyFieldBasedComparator и другим стандартным классам Hadoop есть в Hadoop Java API, например: http://hadoop.apache.org/docs/r2.6.1/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html

    Полностью последовательность команд для сортировки url'ов - в `global_sort.sh`.

9. Как было видно на предыдущем шаге - решение даже несложной задачи может выражаться в  несколько map-reduce шагов. Поэтому важно отслеживать успешность предыдущего шага, чтобы на его результат можно было положиться. Можно проверять код возврата предыдущей команды, в скрипте на bash это можно выразить так:

        # job 1
        hadoop jar …
        if [ “$?” != “0” ]; then
            exit 1
        fi
        # job 2
        hadoop jar …
или

        hadoop jar … && hadoop jar … && echo “OK”
Другой вариант - проверять наличия файла `_SUCCESS` - его Hadoop всегда кладет в выходную директорию задачи:
        # job 1
        hadoop jar …
        if [ ! -e out_dir_job1/_SUCCESS ]; then
            exit 1
        fi
        # job 2
        hadoop jar …


    Проверка _SUCCESS:

10. Если нужно, чтобы некий расчет производился регулярно, то удобно написать скрипт запуска и добавить его в `cron`, как рассказывалось вначале. При этом важно:
    1. запускать скрипт несколько раз на случай, если предыдущий запуск не отработал (например, еще не был готов исходный лог)
    2. проверять готовность исходного лога
    2. не запускать расчет, если предыдущий запуск прошел успешно
    3. не запускать скрипт, если он уже запущен
    4. совсем хорошо - пропускать те этапы расчета, которые в предыдущий раз завершились успешно

    Каркас такого скрипта - в `mr_runner.sh`. Он принимает один аргумент - дату, за которую надо посчитать статистику. Проверяет готовность исходного лога по правилу: если есть уже лог за следующий день, значит лог за предыдущий день полностью готов.

    Защита от повторного запуска - по наличию файла и pid процесса и наличия такого процесса (`kill -0`). Перед запуском map-reduce задачи проверяется наличие `_SUCCESS`. Подобные скрипты удобно писать на bash, но можно и на любом другом языке.

